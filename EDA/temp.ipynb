{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for FOMC Meeting Minutes\n",
    "And testing out spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Test out and create a pipeline for the creation of a model for parsing FOMC minutes and creating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt   # Import matplotlib\n",
    "# This line is necessary for the plot to appear in a Jupyter notebook\n",
    "%matplotlib inline\n",
    "# Control the default size of figures in this Jupyter notebook\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots\n",
    "import glob\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from pandas_datareader import data\n",
    "from matplotlib.dates import DateFormatter, WeekdayLocator, DayLocator, MONDAY\n",
    "from matplotlib.finance import candlestick_ohlc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "from FOMC import FOMC\n",
    "from yahoo_finance import Currency, Share\n",
    "from spacy.en import English\n",
    "import pickle\n",
    "import datetime as dt\n",
    "from __future__ import print_function\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting links...\n",
      "There are 167 statements..\n",
      ".....Getting articles - Multi-threaded......\n",
      "............................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "fomc = FOMC()\n",
    "df = fomc.get_statements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to ../data/minutes.pickle\n"
     ]
    }
   ],
   "source": [
    "fomc.pick_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fomc.pick_df('../data/minutes_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(r'../data/minutes_df.pickle', 'rb') as f:\n",
    "    minutes_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(minutes_df.index[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(minutes_df.ix['2017-03-15'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "doc = nlp(unicode(minutes_df.ix['2017-03-15'][0]))\n",
    "doc.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print('new: ', sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VXX = Share('VXX')  # Volatility\n",
    "float(VXX.get_price()) - float(VXX.get_prev_close())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VXX_historical = VXX.get_historical('2009-01-01', '2010-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VXX_historical[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(dt.date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will look at stock prices over the past year, starting at January 1, 2016\n",
    "start = dt.datetime(2014,1,1)\n",
    "end = dt.date.today()\n",
    " \n",
    "# Let's get Apple stock data; Apple's ticker symbol is AAPL\n",
    "# First argument is the series we want, second is the source (\"yahoo\" for Yahoo! Finance), third is the start date, fourth is the end date\n",
    "apple = data.DataReader(\"AAPL\", \"yahoo\", start, end)\n",
    " \n",
    "type(apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pandas_candlestick_ohlc(dat, stick = \"day\", otherseries = None):\n",
    "    \"\"\"\n",
    "    :param dat: pandas DataFrame object with datetime64 index, and float columns \"Open\", \"High\", \"Low\", and \"Close\", likely created via DataReader from \"yahoo\"\n",
    "    :param stick: A string or number indicating the period of time covered by a single candlestick. Valid string inputs include \"day\", \"week\", \"month\", and \"year\", (\"day\" default), and any numeric input indicates the number of trading days included in a period\n",
    "    :param otherseries: An iterable that will be coerced into a list, containing the columns of dat that hold other series to be plotted as lines\n",
    " \n",
    "    This will show a Japanese candlestick plot for stock data stored in dat, also plotting other series if passed.\n",
    "    \"\"\"\n",
    "    mondays = WeekdayLocator(MONDAY)        # major ticks on the mondays\n",
    "    alldays = DayLocator()              # minor ticks on the days\n",
    "    dayFormatter = DateFormatter('%d')      # e.g., 12\n",
    " \n",
    "    # Create a new DataFrame which includes OHLC data for each period specified by stick input\n",
    "    transdat = dat.loc[:,[\"Open\", \"High\", \"Low\", \"Close\"]]\n",
    "    if (type(stick) == str):\n",
    "        if stick == \"day\":\n",
    "            plotdat = transdat\n",
    "            stick = 1 # Used for plotting\n",
    "        elif stick in [\"week\", \"month\", \"year\"]:\n",
    "            if stick == \"week\":\n",
    "                transdat[\"week\"] = pd.to_datetime(transdat.index).map(lambda x: x.isocalendar()[1]) # Identify weeks\n",
    "            elif stick == \"month\":\n",
    "                transdat[\"month\"] = pd.to_datetime(transdat.index).map(lambda x: x.month) # Identify months\n",
    "            transdat[\"year\"] = pd.to_datetime(transdat.index).map(lambda x: x.isocalendar()[0]) # Identify years\n",
    "            grouped = transdat.groupby(list(set([\"year\",stick]))) # Group by year and other appropriate variable\n",
    "            plotdat = pd.DataFrame({\"Open\": [], \"High\": [], \"Low\": [], \"Close\": []}) # Create empty data frame containing what will be plotted\n",
    "            for name, group in grouped:\n",
    "                plotdat = plotdat.append(pd.DataFrame({\"Open\": group.iloc[0,0],\n",
    "                                            \"High\": max(group.High),\n",
    "                                            \"Low\": min(group.Low),\n",
    "                                            \"Close\": group.iloc[-1,3]},\n",
    "                                           index = [group.index[0]]))\n",
    "            if stick == \"week\": stick = 5\n",
    "            elif stick == \"month\": stick = 30\n",
    "            elif stick == \"year\": stick = 365\n",
    " \n",
    "    elif (type(stick) == int and stick >= 1):\n",
    "        transdat[\"stick\"] = [np.floor(i / stick) for i in range(len(transdat.index))]\n",
    "        grouped = transdat.groupby(\"stick\")\n",
    "        plotdat = pd.DataFrame({\"Open\": [], \"High\": [], \"Low\": [], \"Close\": []}) # Create empty data frame containing what will be plotted\n",
    "        for name, group in grouped:\n",
    "            plotdat = plotdat.append(pd.DataFrame({\"Open\": group.iloc[0,0],\n",
    "                                        \"High\": max(group.High),\n",
    "                                        \"Low\": min(group.Low),\n",
    "                                        \"Close\": group.iloc[-1,3]},\n",
    "                                       index = [group.index[0]]))\n",
    " \n",
    "    else:\n",
    "        raise ValueError('Valid inputs to argument \"stick\" include the strings \"day\", \"week\", \"month\", \"year\", or a positive integer')\n",
    " \n",
    " \n",
    "    # Set plot parameters, including the axis object ax used for plotting\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    if plotdat.index[-1] - plotdat.index[0] < pd.Timedelta('730 days'):\n",
    "        weekFormatter = DateFormatter('%b %d')  # e.g., Jan 12\n",
    "        ax.xaxis.set_major_locator(mondays)\n",
    "        ax.xaxis.set_minor_locator(alldays)\n",
    "    else:\n",
    "        weekFormatter = DateFormatter('%b %d, %Y')\n",
    "    ax.xaxis.set_major_formatter(weekFormatter)\n",
    " \n",
    "    ax.grid(True)\n",
    " \n",
    "    # Create the candelstick chart\n",
    "    candlestick_ohlc(ax, list(zip(list(date2num(plotdat.index.tolist())), plotdat[\"Open\"].tolist(), plotdat[\"High\"].tolist(),\n",
    "                      plotdat[\"Low\"].tolist(), plotdat[\"Close\"].tolist())),\n",
    "                      colorup = \"black\", colordown = \"red\", width = stick * .4)\n",
    " \n",
    "    # Plot other series (such as moving averages) as lines\n",
    "    if otherseries != None:\n",
    "        if type(otherseries) != list:\n",
    "            otherseries = [otherseries]\n",
    "        dat.loc[:,otherseries].plot(ax = ax, lw = 1.3, grid = True)\n",
    " \n",
    "    ax.xaxis_date()\n",
    "    ax.autoscale_view()\n",
    "    plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    " \n",
    "    plt.show()\n",
    " \n",
    "apple[\"Adj Close\"].plot(grid = True) # Plot the adjusted closing price of AAPL\n",
    "pandas_candlestick_ohlc(apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(unicode(\"Apples and oranges are similar. Boots and hippos aren't.\"))\n",
    "\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apples.similarity(oranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'They told us to duck.')\n",
    "for word in doc:\n",
    "    print(word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_person_occurences(processed_text):\n",
    "    \"\"\"\n",
    "    Return a list of actors from `doc` with corresponding occurences.\n",
    "    \n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :return: list of tuples in form\n",
    "        [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266)]\n",
    "    \"\"\"\n",
    "    \n",
    "    characters = Counter()\n",
    "    for ent in processed_text.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            characters[ent.lemma_] += 1\n",
    "            \n",
    "    return characters.most_common()\n",
    "\n",
    "def find_place_occurences(processed_text):\n",
    "    characters = Counter()\n",
    "    for ent in processed_text.ents:\n",
    "        if ent.label_ == 'GPE':\n",
    "            characters[ent.lemma_] += 1\n",
    "    return characters.most_common()\n",
    "\n",
    "def find_rate_occurences(processed_text):\n",
    "    characters = Counter()\n",
    "    for ent in processed_text.ents:\n",
    "        if ent.label_ in ['CARDINAL','PERCENT']:\n",
    "            characters[ent.lemma_] += 1\n",
    "    return characters.most_common()\n",
    "\n",
    "def find_date_occurences(processed_text):\n",
    "    characters = Counter()\n",
    "    for ent in processed_text.ents:\n",
    "        if ent.label_ == 'DATE':\n",
    "            characters[ent.lemma_] += 1\n",
    "    return characters.most_common()\n",
    "\n",
    "def find_org_occurences(processed_text):\n",
    "    characters = Counter()\n",
    "    for ent in processed_text.ents:\n",
    "        if ent.label_ == 'ORG':\n",
    "            characters[ent.lemma_] += 1\n",
    "    return characters.most_common()\n",
    "\n",
    "def find_occurences(processed_text, list_):\n",
    "    characters = Counter()\n",
    "    for ent in processed_text.ents:\n",
    "        if ent.label_ in list_:\n",
    "            characters[ent.lemma_] += 1\n",
    "    return characters.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_occurences(doc, ['MONEY','ORG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.lemma_, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(unicode(minutes_df.iloc[0,0]), )\n",
    "find_person_occurences(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process sentences 'Hello, world. Natural Language Processing in 10 lines of code.' using spaCy\n",
    "doc = nlp(u'Hello, world. Natural Language Processing in 10 lines of code.')\n",
    "\n",
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    \n",
    "print()\n",
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(unicode(minutes_df.iloc[0,0]))\n",
    "doc2 = nlp(unicode(minutes_df.iloc[1,0]))\n",
    "doc99 = nlp(unicode(minutes_df.iloc[-1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc1.similarity(doc99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = nlp(unicode('marry'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(unicode(\"her mother was talking to that one person (Lady Lucas) freely, openly, and of nothing else but her expectation that Jane would soon be married to Mr. Bingley.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VERB_LEMMA = \"marry\"\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        print(ent.root.head.lemma_,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price data for engineering the Y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_trend_data(ax, name, series):\n",
    "    ax.plot(series.index, series)\n",
    "    ax.set_title(\"{}\".format(name))\n",
    "def fit_moving_average_trend(series, window=6):\n",
    "    return series.rolling(window=window,center=False).mean()\n",
    "def plot_moving_average_trend(ax, name, series, window=6):\n",
    "    moving_average_trend = fit_moving_average_trend(series, window)\n",
    "    plot_trend_data(ax, name, series)\n",
    "    ax.plot(series.index, moving_average_trend, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices = dict()\n",
    "col_names = ['date', 'open', 'high', 'low', 'close', 'volume', 'count', 'WAP']\n",
    "for filename in glob.glob('../data/*.csv'):\n",
    "    this_file = filename.split('/')[-1].split('.')[0]\n",
    "    prices[this_file] = pd.read_csv(filename, parse_dates=['date'], infer_datetime_format=True,names=col_names).drop_duplicates()\n",
    "    prices[this_file].set_index('date', inplace=True,)\n",
    "    prices[this_file].index = prices[this_file].index.tz_localize('America/Los_Angeles').tz_convert('America/New_York').tz_localize(None)\n",
    "    prices[this_file]['close-MA-4'] = fit_moving_average_trend(prices[this_file]['close'], \n",
    "                                                              window=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prices.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many observations are there in each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in prices.keys():\n",
    "    print(len(prices[key]), \"observations in {}\".format(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets plot the prices to check for anomalies and outlies (usually misprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in prices.keys():\n",
    "    if len(key) > 8:\n",
    "        plt.plot(prices[key].index, prices[key]['close'])\n",
    "        plt.title(key)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in prices.keys():\n",
    "    if key[:3] in ['USD','EUR']:\n",
    "        plt.plot(prices[key].index, prices[key]['close'])\n",
    "        plt.title(key)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, figsize=(14, 6))\n",
    "plot_moving_average_trend(axs[0], 'open', prices['SPY-USD-TRADES']['open'][:100], window=10)\n",
    "plot_moving_average_trend(axs[1], 'high', prices['SPY-USD-TRADES']['high'][:100], window=10)\n",
    "plot_moving_average_trend(axs[2], 'low', prices['SPY-USD-TRADES']['low'][:100], window=10)\n",
    "plot_moving_average_trend(axs[3], 'close', prices['SPY-USD-TRADES']['close'][:100], window=10)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prices['SHY-USD-TRADES']['close'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like data is pretty clean now.  Lets gather the y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_post_FOMC_time_selector = []\n",
    "for date in minutes_df.index:\n",
    "    pre_post_FOMC_time_selector.extend(pd.date_range(date.replace(hour=13, minute=30), periods=2, freq='2 H'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices_FOMC = dict()\n",
    "for key in prices.keys():\n",
    "    prices_FOMC[key] = prices[key].loc[pre_post_FOMC_time_selector][['close-MA-4']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prices_FOMC['SHY-USD-TRADES'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "this_df = prices_FOMC['SHY-USD-TRADES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = this_df.groupby(this_df.index.date).diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(y > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(y < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We seem to have a pretty balanced number of up days and down days.  \n",
    "\n",
    "Now lets convert everything to target variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_dfs = dict()\n",
    "for key in prices_FOMC:\n",
    "    y_dfs[key] = prices_FOMC[key].groupby(prices_FOMC[key].index.date).diff().dropna() \n",
    "    y_dfs[key]['fomc-close-MA-4-pct'] = y_dfs[key]['close-MA-4'] / prices[key].loc[y_dfs[key].index]['close']\n",
    "    y_dfs[key].index = y_dfs[key].index.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_dfs['SPY-USD-TRADES']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to TF-IDF!!!\n",
    "Lets build the X matrix first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(minutes_df['statements'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minutes_list = minutes_df['statements'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minutes_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minutes_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minutes_df.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(minutes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svn_grid = {'C':[0.01,0.1,1,8,9,10,11,12,15,30,15000],\n",
    "            'gamma':[0.1,1,2,3,4,5]}\n",
    "\n",
    "svn_gridsearch = GridSearchCV(rbf, svn_grid, n_jobs=-1, verbose=True, scoring=\"accuracy\", cv=10)\n",
    "fit = svn_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Digging into the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unicode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c4b10730261e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unicode' is not defined"
     ]
    }
   ],
   "source": [
    "string = \"{}/{}\".format((1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1/2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
